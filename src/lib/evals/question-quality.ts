import { Opik } from 'opik';

const opik = new Opik({ projectName: 'healthic-evals' });

// Question quality evaluation prompt template
const QUESTION_QUALITY_PROMPT = `You are evaluating the quality of follow-up questions generated by a health coach agent.

When a user sends a message, the agent sometimes generates follow-up questions to better understand the user's situation before giving advice. You are judging whether these questions demonstrate good understanding of the user's intent and whether the questions themselves are useful.

Evaluate on these criteria:

1. INTENT UNDERSTANDING: Do the questions show the agent correctly understood what the user is asking about or trying to achieve?
2. RELEVANCE: Are the questions directly related to the user's message, not generic or off-topic?
3. OPTION QUALITY: Are the multiple-choice options meaningful, covering a realistic range of possibilities without being redundant?
4. SPECIFICITY: Are the questions specific enough to gather actionable information, not vague or overly broad?
5. COMPLETENESS: Do the questions cover the important dimensions needed to give good advice for this particular request?

BAD question examples:
- Generic questions that could apply to any health topic ("How motivated are you?")
- Questions with obvious/useless options ("Do you want to be healthy? Yes / No")
- Questions that miss the user's actual intent entirely
- Redundant questions that ask the same thing differently

GOOD question examples:
- Questions that dig into the user's specific situation ("What does your current exercise routine look like?")
- Options that represent real, distinct scenarios the user might be in
- Questions that would meaningfully change the advice given based on the answer

User's message: {input}
Agent's response: {output}
Questions generated: {questions}

Score on a scale of 0.0 to 1.0:
- 0.0: Questions are irrelevant, generic, or show no understanding of intent
- 0.3: Questions are somewhat related but vague or miss the point
- 0.6: Decent questions but could be more targeted or options could be better
- 0.8: Good questions that show clear intent understanding and gather useful info
- 1.0: Excellent, highly specific questions perfectly tailored to the user's situation

Respond in JSON format:
{
  "score": <number between 0 and 1>,
  "reason": "<brief explanation of why this score was given>",
  "criteria": {
    "intentUnderstanding": <number between 0 and 1>,
    "relevance": <number between 0 and 1>,
    "optionQuality": <number between 0 and 1>,
    "specificity": <number between 0 and 1>,
    "completeness": <number between 0 and 1>
  },
  "concerns": ["<list any specific concerns about the questions>"]
}`;

interface QuestionQualityInput {
  input: string;   // User's message
  output: string;  // Agent's response text
  questions: {     // The generated choicesData
    title: string;
    questions: Array<{
      id: string;
      question: string;
      options: string[];
    }>;
  };
}

interface QuestionQualityResult {
  score: number;
  reason: string;
  criteria: {
    intentUnderstanding: number;
    relevance: number;
    optionQuality: number;
    specificity: number;
    completeness: number;
  };
  concerns: string[];
}

/**
 * Evaluates the quality of follow-up questions generated by the agent.
 * Uses LLM-as-judge pattern with the Gemini model.
 */
export async function evaluateQuestionQuality(
  input: QuestionQualityInput
): Promise<QuestionQualityResult> {
  const trace = opik.trace({
    name: 'question_quality_eval',
    input: { userMessage: input.input, agentResponse: input.output, questions: input.questions },
    metadata: { evalType: 'question_quality' },
  });

  try {
    const questionsStr = input.questions.questions
      .map((q, i) => `Q${i + 1}: ${q.question}\nOptions: ${q.options.join(' | ')}`)
      .join('\n\n');

    const prompt = QUESTION_QUALITY_PROMPT
      .replace('{input}', input.input)
      .replace('{output}', input.output)
      .replace('{questions}', questionsStr);

    const response = await fetch(
      'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent',
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-goog-api-key': process.env.GOOGLE_GENAI_API_KEY || '',
        },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }],
          generationConfig: {
            temperature: 0.1,
            responseMimeType: 'application/json',
          },
        }),
      }
    );

    const data = await response.json();
    const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '{}';

    const result: QuestionQualityResult = JSON.parse(text);

    trace.update({
      output: result as unknown as Record<string, unknown>,
      metadata: { status: 'success', score: result.score },
    });
    trace.end();
    await opik.flush();

    return result;
  } catch (error) {
    trace.update({
      output: { error: String(error) },
      metadata: { status: 'error' },
    });
    trace.end();
    await opik.flush();

    throw error;
  }
}

// Test data for running the evaluation
export const questionQualityTestCases = [
  {
    name: 'generic_questions',
    input: 'I want to start running',
    output: 'Great! Let me ask you a few questions to help you get started.',
    questions: {
      title: 'Getting Started',
      questions: [
        { id: '1', question: 'How motivated are you?', options: ['Very', 'Somewhat', 'Not much', 'Other'] },
        { id: '2', question: 'Do you like exercise?', options: ['Yes', 'No', 'Other'] },
      ],
    },
    expectedScoreRange: [0, 0.4], // Should score low - generic, not useful
  },
  {
    name: 'specific_running_questions',
    input: 'I want to start running',
    output: 'I\'d love to help you start running! Let me understand your situation better.',
    questions: {
      title: 'Your Running Plan',
      questions: [
        { id: '1', question: 'What\'s your current fitness level?', options: ['Sedentary (little to no exercise)', 'Light activity (walking regularly)', 'Moderately active (exercise 2-3x/week)', 'Very active (exercise 4+x/week)', 'Other'] },
        { id: '2', question: 'Do you have a specific running goal?', options: ['Run a 5K', 'Run for general fitness', 'Lose weight through running', 'Train for a longer race', 'Other'] },
        { id: '3', question: 'Do you have any injuries or physical limitations?', options: ['No limitations', 'Knee/joint issues', 'Back problems', 'Other health concern', 'Other'] },
      ],
    },
    expectedScoreRange: [0.7, 1.0], // Should score high - specific, relevant, good options
  },
  {
    name: 'misunderstood_intent',
    input: 'I\'m stressed about my diet and nothing seems to work',
    output: 'Let me help you with your diet!',
    questions: {
      title: 'Diet Plan',
      questions: [
        { id: '1', question: 'How many calories do you want to eat?', options: ['1200', '1500', '1800', '2000', 'Other'] },
        { id: '2', question: 'Which diet do you prefer?', options: ['Keto', 'Paleo', 'Vegan', 'Mediterranean', 'Other'] },
      ],
    },
    expectedScoreRange: [0, 0.4], // Should score low - missed the emotional/stress aspect entirely
  },
];

/**
 * Run all question quality test cases and return results.
 */
export async function runQuestionQualityEval(): Promise<{
  passed: number;
  failed: number;
  results: Array<{ name: string; score: number; reason: string; concerns: string[]; passed: boolean }>;
}> {
  const results: Array<{ name: string; score: number; reason: string; concerns: string[]; passed: boolean }> = [];

  for (const testCase of questionQualityTestCases) {
    try {
      const result = await evaluateQuestionQuality({
        input: testCase.input,
        output: testCase.output,
        questions: testCase.questions,
      });

      const passed = result.score >= testCase.expectedScoreRange[0] &&
                     result.score <= testCase.expectedScoreRange[1];

      results.push({
        name: testCase.name,
        score: result.score,
        reason: result.reason,
        concerns: result.concerns || [],
        passed,
      });
    } catch (error) {
      results.push({
        name: testCase.name,
        score: 0,
        reason: `Error: ${error}`,
        concerns: [],
        passed: false,
      });
    }
  }

  return {
    passed: results.filter(r => r.passed).length,
    failed: results.filter(r => !r.passed).length,
    results,
  };
}
